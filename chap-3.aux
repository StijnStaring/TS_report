\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Forecasting the daily electricity consumption}{31}{chapter.4}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cha:Forecasting the daily electricity consumption}{{\M@TitleReference {4}{Forecasting the daily electricity consumption}}{31}{Forecasting the daily electricity consumption}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Pre-processing}{31}{section.4.1}\protected@file@percent }
\newlabel{s:Pre-processing}{{\M@TitleReference {4.1}{Pre-processing}}{31}{Pre-processing}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Data}{31}{subsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The consumption of $ 2017 $ for the three selected series. \relax }}{32}{figure.caption.33}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces summarizing characteristics about the selected series.\relax }}{32}{table.caption.34}\protected@file@percent }
\newlabel{tab:summ_data}{{\M@TitleReference {4.1}{summarizing characteristics about the selected series.\relax }}{32}{summarizing characteristics about the selected series.\relax }{table.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Specifications of different CPU's and GPU tried.\relax }}{33}{table.caption.35}\protected@file@percent }
\newlabel{tab:CPU}{{\M@TitleReference {4.2}{Specifications of different CPU's and GPU tried.\relax }}{33}{Specifications of different CPU's and GPU tried.\relax }{table.caption.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Baseline models}{33}{section.4.2}\protected@file@percent }
\newlabel{s:Baseline models}{{\M@TitleReference {4.2}{Baseline models}}{33}{Baseline models}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Models}{33}{subsection.4.2.1}\protected@file@percent }
\citation{Kong2019}
\newlabel{eq:model_mape}{{4.1}{34}{Models}{equation.4.2.1}{}}
\newlabel{eq:MSE}{{4.2}{35}{Models}{equation.4.2.2}{}}
\newlabel{eq:MAPE}{{4.3}{35}{Models}{equation.4.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Results of baseline models}{35}{subsection.4.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Evaluation results for Serie $ 1 $ tested on $ 31 $ days of December.\relax }}{35}{table.caption.36}\protected@file@percent }
\newlabel{tab:summ_data_serie1}{{\M@TitleReference {4.3}{Evaluation results for Serie $ 1 $ tested on $ 31 $ days of December.\relax }}{35}{Evaluation results for Serie $ 1 $ tested on $ 31 $ days of December.\relax }{table.caption.36}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Evaluation results for Serie $ 2 $ tested on $ 12 $ days of December.\relax }}{36}{table.caption.37}\protected@file@percent }
\newlabel{tab:summ_data_serie2}{{\M@TitleReference {4.4}{Evaluation results for Serie $ 2 $ tested on $ 12 $ days of December.\relax }}{36}{Evaluation results for Serie $ 2 $ tested on $ 12 $ days of December.\relax }{table.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Evaluation results for Serie $ 3 $ tested on $ 12 $ days of December.\relax }}{36}{table.caption.38}\protected@file@percent }
\newlabel{tab:summ_data_serie3}{{\M@TitleReference {4.5}{Evaluation results for Serie $ 3 $ tested on $ 12 $ days of December.\relax }}{36}{Evaluation results for Serie $ 3 $ tested on $ 12 $ days of December.\relax }{table.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Daily predictions of two baseline models. (Blue: True / Orange: Prediction) \relax }}{36}{figure.caption.39}\protected@file@percent }
\newlabel{fig:baseline models 4 and 5}{{\M@TitleReference {4.2}{Daily predictions of two baseline models. (Blue: True / Orange: Prediction) \relax }}{36}{Daily predictions of two baseline models. (Blue: True / Orange: Prediction) \relax }{figure.caption.39}{}}
\citation{loadforecastingmoor}
\citation{Kong2019}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Relative performance over all the $ 261 $ time series.\relax }}{37}{table.caption.40}\protected@file@percent }
\newlabel{tab:summ_data_rel_performance}{{\M@TitleReference {4.6}{Relative performance over all the $ 261 $ time series.\relax }}{37}{Relative performance over all the $ 261 $ time series.\relax }{table.caption.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Neural network models}{37}{section.4.3}\protected@file@percent }
\newlabel{s:Neural network models}{{\M@TitleReference {4.3}{Neural network models}}{37}{Neural network models}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Different practical considerations of the models in Keras}{37}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Inputs}{37}{section*.41}\protected@file@percent }
\newlabel{s:Inputs}{{\M@TitleReference {4.3.1}{Inputs}}{37}{Inputs}{section*.41}{}}
\citation{FneishMo}
\@writefile{toc}{\contentsline {subsubsection}{Batch size}{38}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stateless versus Stateful}{38}{section*.43}\protected@file@percent }
\citation{FneishMo}
\citation{FneishMo}
\citation{FneishMo}
\citation{FneishMo}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The generation of inputs for a stateless model. (source: \cite  {FneishMo})\relax }}{39}{figure.caption.44}\protected@file@percent }
\newlabel{fig:stateless_input}{{\M@TitleReference {4.3}{The generation of inputs for a stateless model. (source: \cite  {FneishMo})\relax }}{39}{The generation of inputs for a stateless model. (source: \cite {FneishMo})\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The generation of inputs for a stateful model. (source: \cite  {FneishMo})\relax }}{40}{figure.caption.45}\protected@file@percent }
\newlabel{fig:stateful_input}{{\M@TitleReference {4.4}{The generation of inputs for a stateful model. (source: \cite  {FneishMo})\relax }}{40}{The generation of inputs for a stateful model. (source: \cite {FneishMo})\relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{Initialization}{40}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Overfitting avoidance in Keras}{40}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Metrics to evaluate the models}{40}{section*.48}\protected@file@percent }
\citation{ANNRNN}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Deep LSTM}{41}{subsection.4.3.2}\protected@file@percent }
\newlabel{s:LSTM_implementation}{{\M@TitleReference {4.3.2}{Deep LSTM}}{41}{Deep LSTM}{subsection.4.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The flow of functions that are executed in order to produce daily forecasts.\relax }}{41}{figure.caption.49}\protected@file@percent }
\newlabel{fig:model1}{{\M@TitleReference {4.5}{The flow of functions that are executed in order to produce daily forecasts.\relax }}{41}{The flow of functions that are executed in order to produce daily forecasts.\relax }{figure.caption.49}{}}
\citation{Kong2019}
\@writefile{toc}{\contentsline {subsubsection}{Model 1: Stateless with no flatten layer}{42}{section*.50}\protected@file@percent }
\newlabel{s:Model1}{{\M@TitleReference {4.3.2}{Model 1: Stateless with no flatten layer}}{42}{Model 1: Stateless with no flatten layer}{section*.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Model 1 - stateless model with as input a subserie of N time steps and $ \bm  {C}_{i} \in \mathbb  {R}^{m} $, $ \bm  {H}_{j} \in \mathbb  {R}^{n} $, $ \bm  {X}_{k} \in \mathbb  {R}^{l} $, $ \hat  {y} \in \mathbb  {R}^{1} $.\relax }}{42}{figure.caption.51}\protected@file@percent }
\newlabel{fig:model1}{{\M@TitleReference {4.6}{Model 1 - stateless model with as input a subserie of N time steps and $ \bm  {C}_{i} \in \mathbb  {R}^{m} $, $ \bm  {H}_{j} \in \mathbb  {R}^{n} $, $ \bm  {X}_{k} \in \mathbb  {R}^{l} $, $ \hat  {y} \in \mathbb  {R}^{1} $.\relax }}{42}{Model 1 - stateless model with as input a subserie of N time steps and $ \bm {C}_{i} \in \mathbb {R}^{m} $, $ \bm {H}_{j} \in \mathbb {R}^{n} $, $ \bm {X}_{k} \in \mathbb {R}^{l} $, $ \hat {y} \in \mathbb {R}^{1} $.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Model 2 - stateless model with as input a subserie of N time steps and $ \bm  {C}_{i} \in \mathbb  {R}^{m} $, $ \bm  {H}_{j} \in \mathbb  {R}^{n} $, $ \bm  {X}_{k} \in \mathbb  {R}^{l} $, $ \hat  {y} \in \mathbb  {R}^{1} $.\relax }}{43}{figure.caption.52}\protected@file@percent }
\newlabel{fig:model2}{{\M@TitleReference {4.7}{Model 2 - stateless model with as input a subserie of N time steps and $ \bm  {C}_{i} \in \mathbb  {R}^{m} $, $ \bm  {H}_{j} \in \mathbb  {R}^{n} $, $ \bm  {X}_{k} \in \mathbb  {R}^{l} $, $ \hat  {y} \in \mathbb  {R}^{1} $.\relax }}{43}{Model 2 - stateless model with as input a subserie of N time steps and $ \bm {C}_{i} \in \mathbb {R}^{m} $, $ \bm {H}_{j} \in \mathbb {R}^{n} $, $ \bm {X}_{k} \in \mathbb {R}^{l} $, $ \hat {y} \in \mathbb {R}^{1} $.\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Model 2: Stateless with flatten layer}{43}{subsection.4.3.3}\protected@file@percent }
\newlabel{s:Model2}{{\M@TitleReference {4.3.3}{Model 2: Stateless with flatten layer}}{43}{Model 2: Stateless with flatten layer}{subsection.4.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Model 3: Stateful}{43}{subsection.4.3.4}\protected@file@percent }
\newlabel{s:Model3}{{\M@TitleReference {4.3.4}{Model 3: Stateful}}{43}{Model 3: Stateful}{subsection.4.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Model 3 - stateful model that connects single LSTM blocks and $ \bm  {C}_{i} \in \mathbb  {R}^{m} $, $ \bm  {H}_{j} \in \mathbb  {R}^{n} $, $ \bm  {X}_{k} \in \mathbb  {R}^{l} $, $ \hat  {y} \in \mathbb  {R}^{1} $.\relax }}{43}{figure.caption.53}\protected@file@percent }
\newlabel{fig:model3}{{\M@TitleReference {4.8}{Model 3 - stateful model that connects single LSTM blocks and $ \bm  {C}_{i} \in \mathbb  {R}^{m} $, $ \bm  {H}_{j} \in \mathbb  {R}^{n} $, $ \bm  {X}_{k} \in \mathbb  {R}^{l} $, $ \hat  {y} \in \mathbb  {R}^{1} $.\relax }}{43}{Model 3 - stateful model that connects single LSTM blocks and $ \bm {C}_{i} \in \mathbb {R}^{m} $, $ \bm {H}_{j} \in \mathbb {R}^{n} $, $ \bm {X}_{k} \in \mathbb {R}^{l} $, $ \hat {y} \in \mathbb {R}^{1} $.\relax }{figure.caption.53}{}}
\citation{Shi2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Parameter search}{44}{subsection.4.3.5}\protected@file@percent }
\newlabel{s:Parameter search}{{\M@TitleReference {4.3.5}{Parameter search}}{44}{Parameter search}{subsection.4.3.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Parameters used during phase 1 for the two stateless models.\relax }}{45}{table.caption.54}\protected@file@percent }
\newlabel{tab:para_phase1}{{\M@TitleReference {4.7}{Parameters used during phase 1 for the two stateless models.\relax }}{45}{Parameters used during phase 1 for the two stateless models.\relax }{table.caption.54}{}}
\citation{Greff2017}
\citation{Greff2017}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Different regularization added during phase 2.\relax }}{46}{table.caption.55}\protected@file@percent }
\newlabel{tab:regulation}{{\M@TitleReference {4.8}{Different regularization added during phase 2.\relax }}{46}{Different regularization added during phase 2.\relax }{table.caption.55}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model 1: Stateless with no flatten layer}{46}{section*.56}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 1 $ of the parameter search. For example: when $ 20 $ LSTM units are chosen, this on average gave a $ 12.08\% $ lower MAE.\relax }}{47}{table.caption.57}\protected@file@percent }
\newlabel{tab:relative_performance_parameters_phase_one_model_one}{{\M@TitleReference {4.9}{Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 1 $ of the parameter search. For example: when $ 20 $ LSTM units are chosen, this on average gave a $ 12.08\% $ lower MAE.\relax }}{47}{Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 1 $ of the parameter search. For example: when $ 20 $ LSTM units are chosen, this on average gave a $ 12.08\% $ lower MAE.\relax }{table.caption.57}{}}
\citation{Greff2017}
\@writefile{lot}{\contentsline {table}{\numberline {4.10}{\ignorespaces The values of the parameters that performed best for the three time series during phase $ 1 $ using model $ 1 $ based on the smallest sum of MAE errors during three runs.\relax }}{48}{table.caption.58}\protected@file@percent }
\newlabel{tab:best_performing_para_phase1}{{\M@TitleReference {4.10}{The values of the parameters that performed best for the three time series during phase $ 1 $ using model $ 1 $ based on the smallest sum of MAE errors during three runs.\relax }}{48}{The values of the parameters that performed best for the three time series during phase $ 1 $ using model $ 1 $ based on the smallest sum of MAE errors during three runs.\relax }{table.caption.58}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model 2: Stateless with flatten layer}{48}{section*.61}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Model 3: Stateful model (1 timestep)}{48}{section*.66}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit  {r\_D}: regularization size of weights DENSE layer, \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM, \textit  {d\_L}: dropout rate of input connections LSTM, \textit  {r\_d\_L}: dropout rate of recurrent connections LSTM, \textit  {d\_D}: dropout rate of DENSE layer input connections, \textit  {or}: best performing serie from phase one)\relax }}{49}{figure.caption.59}\protected@file@percent }
\newlabel{fig:sensitivity_model1}{{\M@TitleReference {4.9}{Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit  {r\_D}: regularization size of weights DENSE layer, \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM, \textit  {d\_L}: dropout rate of input connections LSTM, \textit  {r\_d\_L}: dropout rate of recurrent connections LSTM, \textit  {d\_D}: dropout rate of DENSE layer input connections, \textit  {or}: best performing serie from phase one)\relax }}{49}{Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit {r\_D}: regularization size of weights DENSE layer, \textit {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit {r\_L}: regularization size of input weights of LSTM, \textit {d\_L}: dropout rate of input connections LSTM, \textit {r\_d\_L}: dropout rate of recurrent connections LSTM, \textit {d\_D}: dropout rate of DENSE layer input connections, \textit {or}: best performing serie from phase one)\relax }{figure.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces The evaluation of the error on the validation set in function of the learning rate size.\relax }}{50}{figure.caption.60}\protected@file@percent }
\newlabel{fig:learning_rate_model1}{{\M@TitleReference {4.10}{The evaluation of the error on the validation set in function of the learning rate size.\relax }}{50}{The evaluation of the error on the validation set in function of the learning rate size.\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Conclusion}{50}{section.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.11}{\ignorespaces Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 2 $ of the parameter search.\relax }}{51}{table.caption.62}\protected@file@percent }
\newlabel{tab:relative_performance_parameters_phase_one_model_two}{{\M@TitleReference {4.11}{Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 2 $ of the parameter search.\relax }}{51}{Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 2 $ of the parameter search.\relax }{table.caption.62}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.12}{\ignorespaces The values of the parameters that performed best for the three time series during phase $ 1 $ using model $ 2 $.\relax }}{51}{table.caption.63}\protected@file@percent }
\newlabel{tab:best_performing_para_phase1_model2}{{\M@TitleReference {4.12}{The values of the parameters that performed best for the three time series during phase $ 1 $ using model $ 2 $.\relax }}{51}{The values of the parameters that performed best for the three time series during phase $ 1 $ using model $ 2 $.\relax }{table.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit  {r\_D}: regularization size of weights DENSE layer, \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM, \textit  {d\_L}: dropout rate of input connections LSTM, \textit  {r\_d\_L}: dropout rate of recurrent connections LSTM, \textit  {d\_D}: dropout rate of DENSE layer input connections, \textit  {or}: best performing serie from phase one)\relax }}{52}{figure.caption.64}\protected@file@percent }
\newlabel{fig:sensitivity_model2}{{\M@TitleReference {4.11}{Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit  {r\_D}: regularization size of weights DENSE layer, \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM, \textit  {d\_L}: dropout rate of input connections LSTM, \textit  {r\_d\_L}: dropout rate of recurrent connections LSTM, \textit  {d\_D}: dropout rate of DENSE layer input connections, \textit  {or}: best performing serie from phase one)\relax }}{52}{Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit {r\_D}: regularization size of weights DENSE layer, \textit {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit {r\_L}: regularization size of input weights of LSTM, \textit {d\_L}: dropout rate of input connections LSTM, \textit {r\_d\_L}: dropout rate of recurrent connections LSTM, \textit {d\_D}: dropout rate of DENSE layer input connections, \textit {or}: best performing serie from phase one)\relax }{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces The evaluation of the error on the validation set in function of the learning rate size.\relax }}{53}{figure.caption.65}\protected@file@percent }
\newlabel{fig:learning_rate_model1}{{\M@TitleReference {4.12}{The evaluation of the error on the validation set in function of the learning rate size.\relax }}{53}{The evaluation of the error on the validation set in function of the learning rate size.\relax }{figure.caption.65}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.13}{\ignorespaces Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 1 $ of the parameter search.\relax }}{54}{table.caption.67}\protected@file@percent }
\newlabel{tab:relative_performance_parameters_phase_one_model_three}{{\M@TitleReference {4.13}{Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 1 $ of the parameter search.\relax }}{54}{Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 1 $ of the parameter search.\relax }{table.caption.67}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.14}{\ignorespaces The values of the parameters that performed best for the three time series during phase $ 1 $ using model $3 $.\relax }}{54}{table.caption.68}\protected@file@percent }
\newlabel{tab:best_performing_para_phase1_model3}{{\M@TitleReference {4.14}{The values of the parameters that performed best for the three time series during phase $ 1 $ using model $3 $.\relax }}{54}{The values of the parameters that performed best for the three time series during phase $ 1 $ using model $3 $.\relax }{table.caption.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM and \textit  {or}: best performing serie from phase one)\relax }}{55}{figure.caption.69}\protected@file@percent }
\newlabel{fig:sensitivity_model3}{{\M@TitleReference {4.13}{Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM and \textit  {or}: best performing serie from phase one)\relax }}{55}{Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit {r\_L}: regularization size of input weights of LSTM and \textit {or}: best performing serie from phase one)\relax }{figure.caption.69}{}}
\@setckpt{chap-3}{
\setcounter{page}{56}
\setcounter{equation}{3}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{0}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{maxsecnumdepth}{2}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{68}
\setcounter{lastsheet}{88}
\setcounter{lastpage}{76}
\setcounter{figure}{13}
\setcounter{lofdepth}{1}
\setcounter{table}{14}
\setcounter{lotdepth}{1}
\setcounter{Item}{5}
\setcounter{Hfootnote}{0}
\setcounter{memhycontfloat}{0}
\setcounter{Hpagenote}{0}
\setcounter{bookmark@seq@number}{23}
\setcounter{parentequation}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{0}
\setcounter{algocfproc}{0}
\setcounter{algocf}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{3}
\setcounter{subtable}{0}
\setcounter{section@level}{1}
}
