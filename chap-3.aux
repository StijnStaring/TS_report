\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Forecasting the daily electricity consumption}{31}{chapter.4}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cha:Forecasting the daily electricity consumption}{{\M@TitleReference {4}{Forecasting the daily electricity consumption}}{31}{Forecasting the daily electricity consumption}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Pre-processing}{31}{section.4.1}\protected@file@percent }
\newlabel{s:Pre-processing}{{\M@TitleReference {4.1}{Pre-processing}}{31}{Pre-processing}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Data}{31}{subsection.4.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The consumption of $ 2017 $ for the three selected series. \relax }}{32}{figure.caption.33}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces summarizing characteristics about the selected series.\relax }}{32}{table.caption.34}\protected@file@percent }
\newlabel{tab:summ_data}{{\M@TitleReference {4.1}{summarizing characteristics about the selected series.\relax }}{32}{summarizing characteristics about the selected series.\relax }{table.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Specifications of different CPU's and GPU tried.\relax }}{33}{table.caption.35}\protected@file@percent }
\newlabel{tab:CPU}{{\M@TitleReference {4.2}{Specifications of different CPU's and GPU tried.\relax }}{33}{Specifications of different CPU's and GPU tried.\relax }{table.caption.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Baseline models}{33}{section.4.2}\protected@file@percent }
\newlabel{s:Baseline models}{{\M@TitleReference {4.2}{Baseline models}}{33}{Baseline models}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Models}{33}{subsection.4.2.1}\protected@file@percent }
\citation{Kong2019}
\newlabel{eq:model_mape}{{4.1}{34}{Models}{equation.4.2.1}{}}
\newlabel{eq:MSE}{{4.2}{35}{Models}{equation.4.2.2}{}}
\newlabel{eq:MAPE}{{4.3}{35}{Models}{equation.4.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Results of baseline models}{35}{subsection.4.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Evaluation results for Serie $ 1 $ tested on $ 31 $ days of December.\relax }}{35}{table.caption.36}\protected@file@percent }
\newlabel{tab:summ_data_serie1}{{\M@TitleReference {4.3}{Evaluation results for Serie $ 1 $ tested on $ 31 $ days of December.\relax }}{35}{Evaluation results for Serie $ 1 $ tested on $ 31 $ days of December.\relax }{table.caption.36}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Evaluation results for Serie $ 2 $ tested on $ 12 $ days of December.\relax }}{36}{table.caption.37}\protected@file@percent }
\newlabel{tab:summ_data_serie2}{{\M@TitleReference {4.4}{Evaluation results for Serie $ 2 $ tested on $ 12 $ days of December.\relax }}{36}{Evaluation results for Serie $ 2 $ tested on $ 12 $ days of December.\relax }{table.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Evaluation results for Serie $ 3 $ tested on $ 12 $ days of December.\relax }}{36}{table.caption.38}\protected@file@percent }
\newlabel{tab:summ_data_serie3}{{\M@TitleReference {4.5}{Evaluation results for Serie $ 3 $ tested on $ 12 $ days of December.\relax }}{36}{Evaluation results for Serie $ 3 $ tested on $ 12 $ days of December.\relax }{table.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Daily predictions of two baseline models. (Blue: True / Orange: Prediction) \relax }}{36}{figure.caption.39}\protected@file@percent }
\newlabel{fig:baseline models 4 and 5}{{\M@TitleReference {4.2}{Daily predictions of two baseline models. (Blue: True / Orange: Prediction) \relax }}{36}{Daily predictions of two baseline models. (Blue: True / Orange: Prediction) \relax }{figure.caption.39}{}}
\citation{loadforecastingmoor}
\citation{Kong2019}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Relative performance over all the $ 261 $ time series.\relax }}{37}{table.caption.40}\protected@file@percent }
\newlabel{tab:summ_data_rel_performance}{{\M@TitleReference {4.6}{Relative performance over all the $ 261 $ time series.\relax }}{37}{Relative performance over all the $ 261 $ time series.\relax }{table.caption.40}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Neural network models}{37}{section.4.3}\protected@file@percent }
\newlabel{s:Neural network models}{{\M@TitleReference {4.3}{Neural network models}}{37}{Neural network models}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Different practical considerations of the models in Keras}{37}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Inputs}{37}{section*.41}\protected@file@percent }
\citation{FneishMo}
\@writefile{toc}{\contentsline {subsubsection}{Stateless versus Stateful}{38}{section*.42}\protected@file@percent }
\citation{FneishMo}
\citation{FneishMo}
\citation{FneishMo}
\citation{FneishMo}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The generation of inputs for a stateless model. (source: \cite  {FneishMo})\relax }}{39}{figure.caption.43}\protected@file@percent }
\newlabel{fig:stateless_input}{{\M@TitleReference {4.3}{The generation of inputs for a stateless model. (source: \cite  {FneishMo})\relax }}{39}{The generation of inputs for a stateless model. (source: \cite {FneishMo})\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The generation of inputs for a stateful model. (source: \cite  {FneishMo})\relax }}{40}{figure.caption.44}\protected@file@percent }
\newlabel{fig:stateful_input}{{\M@TitleReference {4.4}{The generation of inputs for a stateful model. (source: \cite  {FneishMo})\relax }}{40}{The generation of inputs for a stateful model. (source: \cite {FneishMo})\relax }{figure.caption.44}{}}
\@writefile{toc}{\contentsline {subsubsection}{Initialization}{40}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Overfitting avoidance in Keras}{40}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Metrics to evaluate the models}{40}{section*.47}\protected@file@percent }
\citation{Kong2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Deep LSTM}{41}{subsection.4.3.2}\protected@file@percent }
\newlabel{s:LSTM_implementation}{{\M@TitleReference {4.3.2}{Deep LSTM}}{41}{Deep LSTM}{subsection.4.3.2}{}}
\citation{Greff2017}
\citation{Shi2018}
\citation{Greff2017a}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Deep GRU}{42}{subsection.4.3.3}\protected@file@percent }
\newlabel{s:GRU_implementation}{{\M@TitleReference {4.3.3}{Deep GRU}}{42}{Deep GRU}{subsection.4.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Parameter search}{42}{subsection.4.3.4}\protected@file@percent }
\newlabel{s:Parameter search}{{\M@TitleReference {4.3.4}{Parameter search}}{42}{Parameter search}{subsection.4.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model 1: Stateless with no flatten layer}{42}{section*.48}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 1 $ of the parameter search.\relax }}{43}{table.caption.49}\protected@file@percent }
\newlabel{tab:relative_performance_parameters_phase_one_model_one}{{\M@TitleReference {4.7}{Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 1 $ of the parameter search.\relax }}{43}{Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 1 $ of the parameter search.\relax }{table.caption.49}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces The values of the parameters that performed best for the three time series during phase $ 1 $ using model $ 1 $ based on the smallest sum of MAE errors during three runs.\relax }}{43}{table.caption.50}\protected@file@percent }
\newlabel{tab:best_performing_para_phase1}{{\M@TitleReference {4.8}{The values of the parameters that performed best for the three time series during phase $ 1 $ using model $ 1 $ based on the smallest sum of MAE errors during three runs.\relax }}{43}{The values of the parameters that performed best for the three time series during phase $ 1 $ using model $ 1 $ based on the smallest sum of MAE errors during three runs.\relax }{table.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit  {r\_D}: regularization size of weights DENSE layer, \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM, \textit  {d\_L}: dropout rate of input connections LSTM, \textit  {r\_d\_L}: dropout rate of recurrent connections LSTM, \textit  {d\_D}: dropout rate of DENSE layer input connections, \textit  {or}: best performing serie from phase one)\relax }}{44}{figure.caption.51}\protected@file@percent }
\newlabel{fig:sensitivity_model1}{{\M@TitleReference {4.5}{Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit  {r\_D}: regularization size of weights DENSE layer, \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM, \textit  {d\_L}: dropout rate of input connections LSTM, \textit  {r\_d\_L}: dropout rate of recurrent connections LSTM, \textit  {d\_D}: dropout rate of DENSE layer input connections, \textit  {or}: best performing serie from phase one)\relax }}{44}{Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit {r\_D}: regularization size of weights DENSE layer, \textit {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit {r\_L}: regularization size of input weights of LSTM, \textit {d\_L}: dropout rate of input connections LSTM, \textit {r\_d\_L}: dropout rate of recurrent connections LSTM, \textit {d\_D}: dropout rate of DENSE layer input connections, \textit {or}: best performing serie from phase one)\relax }{figure.caption.51}{}}
\citation{Greff2017}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The evaluation of the error on the validation set in function of the learning rate size.\relax }}{45}{figure.caption.52}\protected@file@percent }
\newlabel{fig:learning_rate_model1}{{\M@TitleReference {4.6}{The evaluation of the error on the validation set in function of the learning rate size.\relax }}{45}{The evaluation of the error on the validation set in function of the learning rate size.\relax }{figure.caption.52}{}}
\citation{Teuwen2019}
\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces The values of the parameters of model 1 with respect to the different series that are obtained after the three phases of the parameter search.\relax }}{46}{table.caption.53}\protected@file@percent }
\newlabel{tab:best_performing_para_final}{{\M@TitleReference {4.9}{The values of the parameters of model 1 with respect to the different series that are obtained after the three phases of the parameter search.\relax }}{46}{The values of the parameters of model 1 with respect to the different series that are obtained after the three phases of the parameter search.\relax }{table.caption.53}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model 2: Stateless with flatten layer}{46}{section*.54}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.10}{\ignorespaces Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 2 $ of the parameter search.\relax }}{46}{table.caption.55}\protected@file@percent }
\newlabel{tab:relative_performance_parameters_phase_one_model_two}{{\M@TitleReference {4.10}{Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 2 $ of the parameter search.\relax }}{46}{Each value in this table shows the average error when the value of a chosen parameter was used, normalized by the biggest error of the possible values and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst possible value for a chosen parameter for each serie during phase $ 2 $ of the parameter search.\relax }{table.caption.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Bidirectional LSTM}{46}{subsection.4.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Temperature model}{46}{subsection.4.3.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.11}{\ignorespaces The values of the parameters that performed best for the three time series during phase $ 1 $ using model $ 2 $.\relax }}{47}{table.caption.56}\protected@file@percent }
\newlabel{tab:best_performing_para_phase1}{{\M@TitleReference {4.11}{The values of the parameters that performed best for the three time series during phase $ 1 $ using model $ 2 $.\relax }}{47}{The values of the parameters that performed best for the three time series during phase $ 1 $ using model $ 2 $.\relax }{table.caption.56}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.7}Results and evaluation}{47}{subsection.4.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Conclusion}{47}{section.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit  {r\_D}: regularization size of weights DENSE layer, \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM, \textit  {d\_L}: dropout rate of input connections LSTM, \textit  {r\_d\_L}: dropout rate of recurrent connections LSTM, \textit  {d\_D}: dropout rate of DENSE layer input connections, \textit  {or}: best performing serie from phase one)\relax }}{48}{figure.caption.57}\protected@file@percent }
\newlabel{fig:sensitivity_model2}{{\M@TitleReference {4.7}{Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit  {r\_D}: regularization size of weights DENSE layer, \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM, \textit  {d\_L}: dropout rate of input connections LSTM, \textit  {r\_d\_L}: dropout rate of recurrent connections LSTM, \textit  {d\_D}: dropout rate of DENSE layer input connections, \textit  {or}: best performing serie from phase one)\relax }}{48}{Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit {r\_D}: regularization size of weights DENSE layer, \textit {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit {r\_L}: regularization size of input weights of LSTM, \textit {d\_L}: dropout rate of input connections LSTM, \textit {r\_d\_L}: dropout rate of recurrent connections LSTM, \textit {d\_D}: dropout rate of DENSE layer input connections, \textit {or}: best performing serie from phase one)\relax }{figure.caption.57}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces The evaluation of the error on the validation set in function of the learning rate size.\relax }}{49}{figure.caption.58}\protected@file@percent }
\newlabel{fig:learning_rate_model1}{{\M@TitleReference {4.8}{The evaluation of the error on the validation set in function of the learning rate size.\relax }}{49}{The evaluation of the error on the validation set in function of the learning rate size.\relax }{figure.caption.58}{}}
\@setckpt{chap-3}{
\setcounter{page}{50}
\setcounter{equation}{3}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{0}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{maxsecnumdepth}{2}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{62}
\setcounter{lastsheet}{84}
\setcounter{lastpage}{72}
\setcounter{figure}{8}
\setcounter{lofdepth}{1}
\setcounter{table}{11}
\setcounter{lotdepth}{1}
\setcounter{Item}{5}
\setcounter{Hfootnote}{0}
\setcounter{memhycontfloat}{0}
\setcounter{Hpagenote}{0}
\setcounter{bookmark@seq@number}{23}
\setcounter{parentequation}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{0}
\setcounter{algocfproc}{0}
\setcounter{algocf}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{3}
\setcounter{subtable}{0}
\setcounter{section@level}{1}
}
