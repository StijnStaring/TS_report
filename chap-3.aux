\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Forecasting the electricity consumption of individual households}{37}{chapter.4}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{cha:Forecasting the daily electricity consumption}{{\M@TitleReference {4}{Forecasting the electricity consumption of individual households}}{37}{Forecasting the electricity consumption of individual households}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Preprocessing}{37}{section.4.1}\protected@file@percent }
\newlabel{s:Preprocessing_cha4}{{\M@TitleReference {4.1}{Preprocessing}}{37}{Preprocessing}{section.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The electrical consumption in $ 2017 $ for the three selected series. \relax }}{38}{figure.caption.34}\protected@file@percent }
\newlabel{fig:three_series}{{\M@TitleReference {4.1}{The electrical consumption in $ 2017 $ for the three selected series. \relax }}{38}{The electrical consumption in $ 2017 $ for the three selected series. \relax }{figure.caption.34}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Summarizing characteristics about the selected series.\relax }}{38}{table.caption.35}\protected@file@percent }
\newlabel{tab:summ_data}{{\M@TitleReference {4.1}{Summarizing characteristics about the selected series.\relax }}{38}{Summarizing characteristics about the selected series.\relax }{table.caption.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Error metrics}{39}{section.4.2}\protected@file@percent }
\newlabel{s:Error metrics}{{\M@TitleReference {4.2}{Error metrics}}{39}{Error metrics}{section.4.2}{}}
\newlabel{eq:RMSE}{{4.1}{39}{Error metrics}{equation.4.2.1}{}}
\newlabel{eq:NRMSE}{{4.2}{39}{Error metrics}{equation.4.2.2}{}}
\newlabel{eq:MAE}{{4.3}{39}{Error metrics}{equation.4.2.3}{}}
\newlabel{eq:MSE}{{4.4}{39}{Error metrics}{equation.4.2.4}{}}
\newlabel{eq:MAPE}{{4.5}{39}{Error metrics}{equation.4.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Microsoft Azure cloud}{39}{section.4.3}\protected@file@percent }
\newlabel{s:Microsoft Azure cloud}{{\M@TitleReference {4.3}{Microsoft Azure cloud}}{39}{Microsoft Azure cloud}{section.4.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Specifications of different CPU's and GPU used.\relax }}{39}{table.caption.36}\protected@file@percent }
\newlabel{tab:CPU}{{\M@TitleReference {4.2}{Specifications of different CPU's and GPU used.\relax }}{39}{Specifications of different CPU's and GPU used.\relax }{table.caption.36}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Baseline models}{40}{section.4.4}\protected@file@percent }
\newlabel{s:Baseline models}{{\M@TitleReference {4.4}{Baseline models}}{40}{Baseline models}{section.4.4}{}}
\citation{Kong2019}
\citation{Kong2019}
\newlabel{eq:model_mape}{{4.6}{42}{Baseline models}{equation.4.4.6}{}}
\newlabel{eq:model_mape_solve}{{4.7}{43}{Baseline models}{equation.4.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Results of baseline models}{43}{subsection.4.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Baseline results for Serie $ 1 $ tested on $ 31 $ days of December.\relax }}{43}{table.caption.37}\protected@file@percent }
\newlabel{tab:summ_data_serie1}{{\M@TitleReference {4.3}{Baseline results for Serie $ 1 $ tested on $ 31 $ days of December.\relax }}{43}{Baseline results for Serie $ 1 $ tested on $ 31 $ days of December.\relax }{table.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Baseline results for Serie $ 2 $ tested on $ 12 $ days of December.\relax }}{43}{table.caption.38}\protected@file@percent }
\newlabel{tab:summ_data_serie2}{{\M@TitleReference {4.4}{Baseline results for Serie $ 2 $ tested on $ 12 $ days of December.\relax }}{43}{Baseline results for Serie $ 2 $ tested on $ 12 $ days of December.\relax }{table.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Baseline results for Serie $ 3 $ tested on $ 12 $ days of December.\relax }}{43}{table.caption.39}\protected@file@percent }
\newlabel{tab:summ_data_serie3}{{\M@TitleReference {4.5}{Baseline results for Serie $ 3 $ tested on $ 12 $ days of December.\relax }}{43}{Baseline results for Serie $ 3 $ tested on $ 12 $ days of December.\relax }{table.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Daily predictions of two baseline models. (Blue: True / Orange: Prediction) \relax }}{44}{figure.caption.40}\protected@file@percent }
\newlabel{fig:baseline models 4 and 5}{{\M@TitleReference {4.2}{Daily predictions of two baseline models. (Blue: True / Orange: Prediction) \relax }}{44}{Daily predictions of two baseline models. (Blue: True / Orange: Prediction) \relax }{figure.caption.40}{}}
\citation{Shi2018}
\citation{loadforecastingmoor}
\citation{Kong2019}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Relative performance over all the $ 261 $ time series with a full year of measurements.\relax }}{45}{table.caption.41}\protected@file@percent }
\newlabel{tab:summ_data_rel_performance}{{\M@TitleReference {4.6}{Relative performance over all the $ 261 $ time series with a full year of measurements.\relax }}{45}{Relative performance over all the $ 261 $ time series with a full year of measurements.\relax }{table.caption.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Implementation deep LSTM neural network}{45}{section.4.5}\protected@file@percent }
\newlabel{s:Implementation deep LSTM neural network}{{\M@TitleReference {4.5}{Implementation deep LSTM neural network}}{45}{Implementation deep LSTM neural network}{section.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Different practical considerations of the models in Keras}{45}{subsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Inputs}{45}{section*.42}\protected@file@percent }
\newlabel{s:Inputs}{{\M@TitleReference {4.5.1}{Inputs}}{45}{Inputs}{section*.42}{}}
\citation{FneishMo}
\@writefile{toc}{\contentsline {subsubsection}{Batch size}{46}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stateless versus Stateful}{47}{section*.44}\protected@file@percent }
\citation{FneishMo}
\citation{FneishMo}
\citation{FneishMo}
\citation{FneishMo}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The generation of inputs for a stateless model. (source: \cite  {FneishMo})\relax }}{48}{figure.caption.45}\protected@file@percent }
\newlabel{fig:stateless_input}{{\M@TitleReference {4.3}{The generation of inputs for a stateless model. (source: \cite  {FneishMo})\relax }}{48}{The generation of inputs for a stateless model. (source: \cite {FneishMo})\relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{Initialization}{48}{section*.47}\protected@file@percent }
\citation{ANNRNN}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The generation of inputs for a stateful model. (source: \cite  {FneishMo})\relax }}{49}{figure.caption.46}\protected@file@percent }
\newlabel{fig:stateful_input}{{\M@TitleReference {4.4}{The generation of inputs for a stateful model. (source: \cite  {FneishMo})\relax }}{49}{The generation of inputs for a stateful model. (source: \cite {FneishMo})\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsubsection}{Overfitting avoidance in Keras}{49}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Error metrics}{49}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Deep LSTM neural networks}{49}{subsection.4.5.2}\protected@file@percent }
\newlabel{s:Deep LSTM neural networks}{{\M@TitleReference {4.5.2}{Deep LSTM neural networks}}{49}{Deep LSTM neural networks}{subsection.4.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The flow of functions that are executed in order during the prediction process with LSTM models.\relax }}{50}{figure.caption.50}\protected@file@percent }
\newlabel{fig:model1}{{\M@TitleReference {4.5}{The flow of functions that are executed in order during the prediction process with LSTM models.\relax }}{50}{The flow of functions that are executed in order during the prediction process with LSTM models.\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model 1: Stateless with no flatten layer}{50}{section*.51}\protected@file@percent }
\newlabel{s:Model1}{{\M@TitleReference {4.5.2}{Model 1: Stateless with no flatten layer}}{50}{Model 1: Stateless with no flatten layer}{section*.51}{}}
\citation{Kong2019}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Model 1 - stateless model with as input a subserie of N time steps and $ \bm  {C}_{i} \in \mathbb  {R}^{m} $, $ \bm  {H}_{j} \in \mathbb  {R}^{n} $, $ \bm  {X}_{k} \in \mathbb  {R}^{59} $, $ \hat  {y} \in \mathbb  {R}^{1} $.\relax }}{51}{figure.caption.52}\protected@file@percent }
\newlabel{fig:model1}{{\M@TitleReference {4.6}{Model 1 - stateless model with as input a subserie of N time steps and $ \bm  {C}_{i} \in \mathbb  {R}^{m} $, $ \bm  {H}_{j} \in \mathbb  {R}^{n} $, $ \bm  {X}_{k} \in \mathbb  {R}^{59} $, $ \hat  {y} \in \mathbb  {R}^{1} $.\relax }}{51}{Model 1 - stateless model with as input a subserie of N time steps and $ \bm {C}_{i} \in \mathbb {R}^{m} $, $ \bm {H}_{j} \in \mathbb {R}^{n} $, $ \bm {X}_{k} \in \mathbb {R}^{59} $, $ \hat {y} \in \mathbb {R}^{1} $.\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Model 2: Stateless with flatten layer}{51}{subsection.4.5.3}\protected@file@percent }
\newlabel{s:Model2}{{\M@TitleReference {4.5.3}{Model 2: Stateless with flatten layer}}{51}{Model 2: Stateless with flatten layer}{subsection.4.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Model 2 - stateless model with as input a serie of N time steps and $ \bm  {C}_{i} \in \mathbb  {R}^{m} $, $ \bm  {H}_{j} \in \mathbb  {R}^{n} $, $ \bm  {X}_{k} \in \mathbb  {R}^{59} $, $ \hat  {y} \in \mathbb  {R}^{1} $.\relax }}{51}{figure.caption.53}\protected@file@percent }
\newlabel{fig:model2}{{\M@TitleReference {4.7}{Model 2 - stateless model with as input a serie of N time steps and $ \bm  {C}_{i} \in \mathbb  {R}^{m} $, $ \bm  {H}_{j} \in \mathbb  {R}^{n} $, $ \bm  {X}_{k} \in \mathbb  {R}^{59} $, $ \hat  {y} \in \mathbb  {R}^{1} $.\relax }}{51}{Model 2 - stateless model with as input a serie of N time steps and $ \bm {C}_{i} \in \mathbb {R}^{m} $, $ \bm {H}_{j} \in \mathbb {R}^{n} $, $ \bm {X}_{k} \in \mathbb {R}^{59} $, $ \hat {y} \in \mathbb {R}^{1} $.\relax }{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Model 3: Stateful}{51}{subsection.4.5.4}\protected@file@percent }
\newlabel{s:Model3}{{\M@TitleReference {4.5.4}{Model 3: Stateful}}{51}{Model 3: Stateful}{subsection.4.5.4}{}}
\citation{Shi2018}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Model 3 - stateful model that connects single LSTM blocks and $ \bm  {C}_{i} \in \mathbb  {R}^{m} $, $ \bm  {H}_{j} \in \mathbb  {R}^{n} $, $ \bm  {X}_{k} \in \mathbb  {R}^{59} $, $ \hat  {y} \in \mathbb  {R}^{1} $.\relax }}{52}{figure.caption.54}\protected@file@percent }
\newlabel{fig:model3}{{\M@TitleReference {4.8}{Model 3 - stateful model that connects single LSTM blocks and $ \bm  {C}_{i} \in \mathbb  {R}^{m} $, $ \bm  {H}_{j} \in \mathbb  {R}^{n} $, $ \bm  {X}_{k} \in \mathbb  {R}^{59} $, $ \hat  {y} \in \mathbb  {R}^{1} $.\relax }}{52}{Model 3 - stateful model that connects single LSTM blocks and $ \bm {C}_{i} \in \mathbb {R}^{m} $, $ \bm {H}_{j} \in \mathbb {R}^{n} $, $ \bm {X}_{k} \in \mathbb {R}^{59} $, $ \hat {y} \in \mathbb {R}^{1} $.\relax }{figure.caption.54}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Parameter search}{52}{subsection.4.5.5}\protected@file@percent }
\newlabel{s:Parameter search}{{\M@TitleReference {4.5.5}{Parameter search}}{52}{Parameter search}{subsection.4.5.5}{}}
\citation{Mele1993}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Parameters used during phase 1 for the two stateless models.\relax }}{53}{table.caption.55}\protected@file@percent }
\newlabel{tab:para_phase1}{{\M@TitleReference {4.7}{Parameters used during phase 1 for the two stateless models.\relax }}{53}{Parameters used during phase 1 for the two stateless models.\relax }{table.caption.55}{}}
\citation{Greff2017}
\citation{Shi2018}
\citation{Greff2017}
\@writefile{lot}{\contentsline {table}{\numberline {4.8}{\ignorespaces Different regularization added during phase 2.\relax }}{54}{table.caption.56}\protected@file@percent }
\newlabel{tab:regulation}{{\M@TitleReference {4.8}{Different regularization added during phase 2.\relax }}{54}{Different regularization added during phase 2.\relax }{table.caption.56}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model 1: Stateless with no flatten layer}{55}{section*.57}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.9}{\ignorespaces Each value in this table shows the average error when the corresponding parameter value is used, normalized by the biggest error of the possible values of one parameter and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst value for one parameter for each serie during phase $ 1 $ of the parameter search.\relax }}{56}{table.caption.58}\protected@file@percent }
\newlabel{tab:relative_performance_parameters_phase_one_model_one}{{\M@TitleReference {4.9}{Each value in this table shows the average error when the corresponding parameter value is used, normalized by the biggest error of the possible values of one parameter and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst value for one parameter for each serie during phase $ 1 $ of the parameter search.\relax }}{56}{Each value in this table shows the average error when the corresponding parameter value is used, normalized by the biggest error of the possible values of one parameter and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst value for one parameter for each serie during phase $ 1 $ of the parameter search.\relax }{table.caption.58}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.10}{\ignorespaces The values of the parameters with the lowest average MAE on the validation set over three runs.\relax }}{56}{table.caption.59}\protected@file@percent }
\newlabel{tab:best_performing_para_phase1}{{\M@TitleReference {4.10}{The values of the parameters with the lowest average MAE on the validation set over three runs.\relax }}{56}{The values of the parameters with the lowest average MAE on the validation set over three runs.\relax }{table.caption.59}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Results of the sensitivity analysis on the size of the regularization parameter and the dropout rate according to MAE. (Legend: \textit  {r\_D}: regularization size of weights of DENSE layer, \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM, \textit  {d\_L}: dropout rate of inputs LSTM, \textit  {r\_d\_L}: dropout rate of hidden states LSTM, \textit  {d\_D}: dropout rate of DENSE layer, \textit  {or}: best performing serie from phase one)\relax }}{57}{figure.caption.60}\protected@file@percent }
\newlabel{fig:sensitivity_model1}{{\M@TitleReference {4.9}{Results of the sensitivity analysis on the size of the regularization parameter and the dropout rate according to MAE. (Legend: \textit  {r\_D}: regularization size of weights of DENSE layer, \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM, \textit  {d\_L}: dropout rate of inputs LSTM, \textit  {r\_d\_L}: dropout rate of hidden states LSTM, \textit  {d\_D}: dropout rate of DENSE layer, \textit  {or}: best performing serie from phase one)\relax }}{57}{Results of the sensitivity analysis on the size of the regularization parameter and the dropout rate according to MAE. (Legend: \textit {r\_D}: regularization size of weights of DENSE layer, \textit {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit {r\_L}: regularization size of input weights of LSTM, \textit {d\_L}: dropout rate of inputs LSTM, \textit {r\_d\_L}: dropout rate of hidden states LSTM, \textit {d\_D}: dropout rate of DENSE layer, \textit {or}: best performing serie from phase one)\relax }{figure.caption.60}{}}
\citation{Greff2017}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces The MAE on the validation set in function of the learning rate size.\relax }}{58}{figure.caption.61}\protected@file@percent }
\newlabel{fig:learning_rate_model1}{{\M@TitleReference {4.10}{The MAE on the validation set in function of the learning rate size.\relax }}{58}{The MAE on the validation set in function of the learning rate size.\relax }{figure.caption.61}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.11}{\ignorespaces Final values found after the parameter search for model 1.\relax }}{59}{table.caption.62}\protected@file@percent }
\newlabel{tab:final_model1}{{\M@TitleReference {4.11}{Final values found after the parameter search for model 1.\relax }}{59}{Final values found after the parameter search for model 1.\relax }{table.caption.62}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model 2: Stateless with flatten layer}{60}{section*.63}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.12}{\ignorespaces Final values found after the parameter search for model 2.\relax }}{60}{table.caption.64}\protected@file@percent }
\newlabel{tab:final_model2}{{\M@TitleReference {4.12}{Final values found after the parameter search for model 2.\relax }}{60}{Final values found after the parameter search for model 2.\relax }{table.caption.64}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model 3: Stateful model}{60}{section*.65}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Conclusion}{61}{section.4.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.13}{\ignorespaces Final values found after the parameter search for model 3.\relax }}{62}{table.caption.66}\protected@file@percent }
\newlabel{tab:final_model3}{{\M@TitleReference {4.13}{Final values found after the parameter search for model 3.\relax }}{62}{Final values found after the parameter search for model 3.\relax }{table.caption.66}{}}
\@setckpt{chap-3}{
\setcounter{page}{63}
\setcounter{equation}{7}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{0}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{maxsecnumdepth}{2}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{73}
\setcounter{lastsheet}{102}
\setcounter{lastpage}{92}
\setcounter{figure}{10}
\setcounter{lofdepth}{1}
\setcounter{table}{13}
\setcounter{lotdepth}{1}
\setcounter{Item}{2}
\setcounter{Hfootnote}{0}
\setcounter{memhycontfloat}{0}
\setcounter{Hpagenote}{0}
\setcounter{bookmark@seq@number}{23}
\setcounter{parentequation}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{0}
\setcounter{algocfproc}{0}
\setcounter{algocf}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{3}
\setcounter{subtable}{0}
\setcounter{section@level}{1}
}
