\chapter{Model evaluation}
\label{cha:Model evaluation}

This chapter discusses the models that were introduced in Chapter \ref{cha:Forecasting the daily electricity consumption} on the test set. As was shown in Table \ref{tab:summ_data}, the test set consists out of the days of the month December. Missing days in December are removed to avoid the influence of the estimation error of the reference signal. In this chapter first the model selection is explained after which a discussion of the performance on the test set follows.

\section{Model selection}\label{s:Model selection}
From Chapter \ref{cha:Forecasting the daily electricity consumption} the model parameters are tuned, but there is still a factor of random model performance due to the random initialization of the weight matrices. To reduce this influence, the model is trained $ 10 $ times and the model that performed best on a validation set is selected. As validation set the $ 10 $ last days of November are used. Also, during the training early stopping is performed wherefore an additional $ 10\% $ of the training data taken to serve as a second validation set. The patience parameter is taken as $ 5 $, which means that the validation error can increase $ 5 $ times before the model is stopped. The maximum amount of epochs that is allowed is $ 150 $. The values of the parameters for each of the three time series can be found in Chapter \ref{cha:Forecasting the daily electricity consumption}. Table \ref{tab:summ_model_selection} summarizes the results of the model selection. 

\textbf{Add here a table with the model selection results.}
- epochs
- loss on validation set




\subsubsection{Performance on the test set}
In this section the results on the test set are discussed of the model that was selected in Section \ref{s:Model selection}.

- The model, last ten days are used for getting best model run and 10\% of the training is used for early stopping. Then model that get after training is run on the test set. The neural models have the downside in comparison with the baseline models that they have to remove training data that is very close to the test set during training. For example, the base line models could use all the previous data to perform training till the desired day

For example model three, only uses training data till end October. Model one and model two use training data till $ 20 $ November and miss a random $ 10 \%$ of the data during the year. This loss of data was necessary for model tuning and the fact that the LSTM model make generalizations before they know the query i.e. the day to forecast (eager models) and the base models learn from the previous data when they know the query (lazy models).

Say that here use the models that obtained in the previous chapter. Make MSE plot for the different NN and the baseline models for each serie and make a comparison of the MAPE with the other baseline model. (by making use of barplots --> normalized with the worst performer) Performance on the test set. 

Make a plot of the day forecast for each NN model --> four graphs bundled. 
- for comparing performance in the same time serie --> mae is used. To compare performance between different time series --> MAPE is used. 
- for stateless random 10 percent is used as validation set. Remember that should shuffle the training set beforehand otherwise j
- for stateful --> the last 30 days of November will be used --> no shuffling is allowed.

- make a graph which shows all the different days that have to be forecasted on the x-axis and the MAE error on the y-axis for the different models. (line graph)

- give an indication how long the models trained --> amount of epochs. 

- when there are a lot of missing days in the test data: serie 1 (0 days) and the other two (8days) -->it is naturally that the model performs worse, the previous day it bases its forecast on is just an estimate. For the day that is forecasted are always days where the true reference signal is available. 



- bar plot for general performance (MAE and MAPE - basemodels and NN)

- line plot for the error on each day (MAE and MAPE - basemodels and NN)

- the signal for the forecast of a day for each model

- in some plots will see that there are gaps --> reason is that it is not useful to predict on the $ 8 $ missing days in the test set. There will only be an estimated signal available to calculate an error on. 

- the line plot shows the distribution of the error per day on the predictions per day. 

- also the mape metric is considered because it allows to compare between different series. 


\section{Conclusion}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
