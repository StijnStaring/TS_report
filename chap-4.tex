\chapter{Model evaluation}
\label{cha:Model evaluation}

This chapter discusses the performance of the models that were introduced in Chapter \ref{cha:Forecasting the daily electricity consumption} on the test set. As was shown in Table \ref{tab:summ_data}, the test set consists out of the days of the month December. The goal of the test set is to assess the model performance on new data and it therefore important that the models are not trained and their parameters are not tuned with data coming from the test set. Missing days in the test set are removed to avoid the influence of the estimation error of the reference signal on the model performance. In this chapter first the model selection is explained after which a discussion of the performance on the test set follows.

\section{Model selection}\label{s:Model selection}
From Chapter \ref{cha:Forecasting the daily electricity consumption} the model parameters are tuned, but there is still a factor of random model performance due to the random initialization of the weight matrices. To reduce this influence, the model is trained $ 10 $ times and the model that performed best on a validation set using the $ MAE $ metric is selected. As validation set the $ 10 $ last days of November are used. Also, during the training early stopping is applied wherefore an additional $ 10\% $ of the training data is taken to serve as a second validation set. For a stateless model this $ 10\% $ is randomly taken from the remaining training matrix. For a stateful model this $ 10\% $ originates form the end of the training matrix. The patience parameter is taken as $ 5 $, which means that the validation error can increase $ 5 $ times before the model is stopped. The maximum amount of epochs that is allowed is $ 150 $. The values of the parameters for each of the three time series can be found in Chapter \ref{cha:Forecasting the daily electricity consumption}. Table \ref{tab:summ_model_selection} summarizes the amount of epochs that the model that is finally selected has run.

\begin{table}[h]
	\centering
	\begin{tabular}{@{}l|ccc@{}} \toprule
				         & \textbf{Model $ 1 $} & \textbf{Model $ 2 $} & \textbf{Model $ 3 $}\\\midrule
		\textbf{Serie 1} & $5 $&$ 16$  & $5 $\\
		\textbf{Serie 2} & $7 $&$ 150 $  & $150$\\
		\textbf{Serie 3} & $19 $&$ 11 $  & $6$\\\bottomrule
	\end{tabular}
	\caption{The amount of training epochs that each model has trained on the different series.}
	\label{tab:summ_model_selection}
\end{table}



\section{Performance on the test set}
In this section the results on the test set are discussed for the models that were selected in section \ref{s:Model selection}. 
Also the two best baseline models ``mean forecast'' and ``MAPE forecast'', that were explained in Section \ref{s:Baseline models}, are included in the comparison. An advantage that the baseline models have in comparison to the LSTM neural networks is that they use the previous data till the day to forecast to make predictions while the LSTM neural networks only trains on data till November. This is because it needs data for the validation set to tune parameters and implement early stopping. \\
The baseline models and the LSTM neural networks both belong to a different group of models respectively to the lazy models and eager models. A lazy model only looks at the data when the query is known e.g. what day to forecast. For example a ``mean forecast'' looks after it knows which day is the desired day to the same weekdays and takes the average. To the contrary an eager model already makes generalizations on the training set before it knows which day it has to predict for. This applies for the LSTM models.\\

Model 1 and Model 2 make use of a lag value of $ 48 $ or $ 96 $ and no seeding. When the day after a missing day(s) is predicted, the inputs could originate entirely from an estimated reference signal. If this is the case, it is expected that the error on the desired day will be larger. The estimation of the reference signal is done by substituting the missing values as described in Section \ref{s:Data}. For both Serie 1 and Serie 2 there are $ 8 $ missing days during the month December.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{MAE_1.png}
	\caption{The MAE performance on all the days of the test set for Serie 1.}
	\label{fig:MAE_serie1}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{MAE_2.png}
	\caption{The MAE performance on all the days of the test set for Serie 2.}
	\label{fig:MAE_serie2}
\end{figure}	

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{MAE_3.png}
	\caption{The MAE performance on all the days of the test set for Serie 3.}
	\label{fig:MAE_serie3}
\end{figure}	

Figures \ref{fig:MAE_serie1}, \ref{fig:MAE_serie2} and \ref{fig:MAE_serie3} give a comparison of the three LSTM models and the two best baseline models. It can be seen that there is a reduction of the MAE error for Series 1 and 2. To get more insight in how the MAE error is distributed over the test set, it is calculated for each individual day and displayed in Figures \ref{fig:MAE_line_serie1}, \ref{fig:MAE_line_serie2} and \ref{fig:MAE_line_serie3} in Appendix \ref{app:Extensions on the evaluation results}. \ref{fig:MAE_line_serie2} and \ref{fig:MAE_line_serie3} are discontinuous due to the missing days that are present in the reference signal in the month December. When the real true values are not known, the day is removed from the test set. This is also the case for the Figures \ref{fig:MAE_serie1}, \ref{fig:MAE_serie2} and \ref{fig:MAE_serie3}. \\
Next, it is noted that the MAPE error for the LSTM models is much higher than for the baseline models as is showed in Table \ref{tab:MAPE_results}. As is displayed in Figure \ref{fig:individual_forecasts} this is due to an overestimation of the electrical consumption when the real electrical consumption is low.\\

\begin{table}[h]
	\centering
	\begin{tabular}{@{}l|ccccc@{}} \toprule
		&\textbf{Mean forecast} & \textbf{MAPE forecast} & \textbf{Model $ 1 $} & \textbf{Model $ 2 $} & \textbf{Model $ 3 $}\\\midrule
		\textbf{Serie 1} & $0.46 $&$ 0.41$  & $3.07 $ & $3.37 $  & $3.27 $\\
		\textbf{Serie 2} & $1.82 $&$ 0.83 $  & $3.67$ & $3.64 $  & $3.65 $\\
		\textbf{Serie 3} & $0.80 $&$ 0.48 $  & $3.30$ & $3.44 $ & $3.21 $\\\bottomrule
	\end{tabular}
	\caption{The amount of training epochs that each model has trained on the different series.}
	\label{tab:summary_MAPE_error}
\end{table}

To get a better insight of the actual output of the different models, Figure \ref{fig:individual_days_prediction} shows the prediction of the different models on a chosen day in the test set.
 
 \begin{figure}[ht]
 	\begin{subfigure}{0.32\textwidth}
 		\includegraphics[width=1\linewidth]{IDM1_S1_Day341.png}
 		\caption{Model $ 1 $ - Serie $ 1 $}
 	\end{subfigure}	 	
 	\begin{subfigure}{0.32\textwidth}
 		\includegraphics[width=1\linewidth]{IDM1_S2_Day341.png}
 		\caption{Model $ 1 $ - Serie $ 2 $}
 	\end{subfigure}	
 	\begin{subfigure}{0.32\textwidth}
 		\includegraphics[width=1\linewidth]{IDM1_S3_Day341.png}
 		\caption{Model $ 1 $ - Serie $ 3 $}
 	\end{subfigure}
  	\begin{subfigure}{0.32\textwidth}
 		\includegraphics[width=1\linewidth]{IDM2_S1_Day341.png}
 		\caption{Model $ 2 $ - Serie $ 1 $}
 	\end{subfigure}	 	
	 \begin{subfigure}{0.32\textwidth}
	 	\includegraphics[width=1\linewidth]{IDM2_S2_Day341.png}
	 	\caption{Model $ 2 $ - Serie $ 2 $}
	 \end{subfigure}	
	 \begin{subfigure}{0.32\textwidth}
	 	\includegraphics[width=1\linewidth]{IDM2_S3_Day341.png}
	 	\caption{Model $ 2 $ - Serie $ 3 $}
	 \end{subfigure}
 	\begin{subfigure}{0.32\textwidth}
	\includegraphics[width=1\linewidth]{IDM3_S1_Day341.png}
	\caption{Model $3 $ - Serie $ 1 $}
	\end{subfigure}	 	
	\begin{subfigure}{0.32\textwidth}
		\includegraphics[width=1\linewidth]{IDM3_S2_Day341.png}
		\caption{Model $3 $ - Serie $ 2 $}
	\end{subfigure}	
	\begin{subfigure}{0.32\textwidth}
		\includegraphics[width=1\linewidth]{IDM3_S3_Day341.png}
		\caption{Model $3 $ - Serie $ 3 $}
	\end{subfigure}
 	\begin{subfigure}{0.32\textwidth}
		\includegraphics[width=1\linewidth]{IDMean_S1_Day341.png}
		\caption{Mean forecast - Serie $ 1 $}
	\end{subfigure}	 	
	\begin{subfigure}{0.32\textwidth}
		\includegraphics[width=1\linewidth]{IDMean_S2_Day341.png}
		\caption{Mean forecast - Serie $ 2 $}
	\end{subfigure}	
	\begin{subfigure}{0.32\textwidth}
		\includegraphics[width=1\linewidth]{IDMean_S3_Day341.png}
		\caption{Mean forecast - Serie $ 3 $}
	\end{subfigure}
	 \begin{subfigure}{0.32\textwidth}
		\includegraphics[width=1\linewidth]{IDMAPE_S1_Day341.png}
		\caption{MAPE forecast - Serie $ 1 $}
	\end{subfigure}	 	
	\begin{subfigure}{0.32\textwidth}
		\includegraphics[width=1\linewidth]{IDMAPE_S2_Day341.png}
		\caption{MAPE forecast - Serie $ 2 $}
	\end{subfigure}	
	\begin{subfigure}{0.32\textwidth}
		\includegraphics[width=1\linewidth]{IDMAPE_S3_Day341.png}
		\caption{MAPE forecast - Serie $ 3 $}
	\end{subfigure}
 	\caption{The prediction result of the different models on $ 7 $ December. (True Future: blue/ Prediction: orange)}
 	\label{fig:individual_forecasts}
 \end{figure}


First, it is again stressed that the ``mean squared error'' is chosen as error metric during training of the LSTM models. Because of this, the LSTM models are more pushed towards learning the peaks of the reference signal. It can be seen that on the $ 7 $th of December, the peaks of Serie 1 and Serie 3 are present in the predicted signal. Next, it can be seen that especially for Serie 1, the LSTM models are predicting a higher consumption than the reference signal. The shape of the two signals is similar, but there is an offset between the two signals. It can be argued that it is better to predict a higher consumption that one that is too low, because it is better to anticipate to a worse scenario then expected.\\

The reader is reminded that there is only no regulation added for Model 1: Serie 1 and Model 3: Serie 1 and 3. When regularization is added this is clearly visible in Figure \ref{fig:individual_forecasts} because without regulation the predicted signal is much more choppy. A clear example can be seen for Model 2 and 3 in serie 3. These two models both show a small bump in the predicted signal before the larger peak. Model 2 which adds regularization shows a very smooth bump while Model 3 without regularization the bump is much more choppy. \\

It can be seen that the ``mean forecast'' also is able to identify the two peaks in Serie 1 and the big peak in Serie 3, but as is expected from a mean, this peak is smaller due to averaging. The ``mean forecast'' method however suffers less from an offset than the LSTM models do in Serie 1.\\
As expected the ``MAPE forecast'' will focus on correctly predicting the low electrical consumptions because these small values will get in the denominator of the MAPE error metric according to Eq. \ref{eq:MAPE}. It can be seen that the peaks will be ignored when this method is applied. 
\clearpage
\section{Conclusion}


- shape of the graphs is okay --> for practical use, the prediction of peaks is also maybe the most important.
- the prediction is often too high

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
