\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {appendix}{\chapternumberline {B}Forecasting the electricity consumption of individual households - extra}{79}{appendix.B}\protected@file@percent }
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:B}{{\M@TitleReference {B}{Forecasting the electricity consumption of individual households - extra}}{79}{Forecasting the electricity consumption of individual households - extra}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Baseline models}{79}{section.B.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {B.1}{\ignorespaces An example histogram of the consumption in [kWh] versus count [-] used during MAPE forecast.\relax }}{79}{figure.caption.76}\protected@file@percent }
\newlabel{fig:histogram_mape}{{\M@TitleReference {B.1}{An example histogram of the consumption in [kWh] versus count [-] used during MAPE forecast.\relax }}{79}{An example histogram of the consumption in [kWh] versus count [-] used during MAPE forecast.\relax }{figure.caption.76}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Parameter Search}{80}{section.B.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.1}Model 2}{80}{subsection.B.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.1}{\ignorespaces Each value in this table shows the average error when the corresponding parameter value is used, normalized by the largest error of the possible values of one parameter and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst value for one parameter for each serie during phase $ 1 $ of the parameter search.\relax }}{80}{table.caption.77}\protected@file@percent }
\newlabel{tab:relative_performance_parameters_phase_one_model_two}{{\M@TitleReference {B.1}{Each value in this table shows the average error when the corresponding parameter value is used, normalized by the largest error of the possible values of one parameter and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst value for one parameter for each serie during phase $ 1 $ of the parameter search.\relax }}{80}{Each value in this table shows the average error when the corresponding parameter value is used, normalized by the largest error of the possible values of one parameter and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst value for one parameter for each serie during phase $ 1 $ of the parameter search.\relax }{table.caption.77}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.2}{\ignorespaces The values of the parameters with the lowest average MAE on the validation set over three runs.\relax }}{80}{table.caption.78}\protected@file@percent }
\newlabel{tab:best_performing_para_phase1_model2}{{\M@TitleReference {B.2}{The values of the parameters with the lowest average MAE on the validation set over three runs.\relax }}{80}{The values of the parameters with the lowest average MAE on the validation set over three runs.\relax }{table.caption.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.2}{\ignorespaces Results of the sensitivity analysis on the size of the regularization parameter and the dropout rate according to MAE.(Legend: \textit  {r\_D}: regularization size of weights of DENSE layer, \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM, \textit  {d\_L}: dropout rate of inputs LSTM, \textit  {r\_d\_L}: dropout rate of hidden states LSTM, \textit  {d\_D}: dropout rate of DENSE layer, \textit  {or}: best performing serie from phase one)\relax }}{81}{figure.caption.79}\protected@file@percent }
\newlabel{fig:sensitivity_model2}{{\M@TitleReference {B.2}{Results of the sensitivity analysis on the size of the regularization parameter and the dropout rate according to MAE.(Legend: \textit  {r\_D}: regularization size of weights of DENSE layer, \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM, \textit  {d\_L}: dropout rate of inputs LSTM, \textit  {r\_d\_L}: dropout rate of hidden states LSTM, \textit  {d\_D}: dropout rate of DENSE layer, \textit  {or}: best performing serie from phase one)\relax }}{81}{Results of the sensitivity analysis on the size of the regularization parameter and the dropout rate according to MAE.(Legend: \textit {r\_D}: regularization size of weights of DENSE layer, \textit {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit {r\_L}: regularization size of input weights of LSTM, \textit {d\_L}: dropout rate of inputs LSTM, \textit {r\_d\_L}: dropout rate of hidden states LSTM, \textit {d\_D}: dropout rate of DENSE layer, \textit {or}: best performing serie from phase one)\relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.3}{\ignorespaces The evaluation of the error on the validation set in function of the learning rate size.\relax }}{82}{figure.caption.80}\protected@file@percent }
\newlabel{fig:learning_rate_model2}{{\M@TitleReference {B.3}{The evaluation of the error on the validation set in function of the learning rate size.\relax }}{82}{The evaluation of the error on the validation set in function of the learning rate size.\relax }{figure.caption.80}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2.2}Model 3}{83}{subsection.B.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {B.3}{\ignorespaces Each value in this table shows the average error when the corresponding parameter value is used, normalized by the largest error of the possible values of one parameter and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst value for one parameter for each serie during phase $ 1 $ of the parameter search.\relax }}{83}{table.caption.81}\protected@file@percent }
\newlabel{tab:relative_performance_parameters_phase_one_model_three}{{\M@TitleReference {B.3}{Each value in this table shows the average error when the corresponding parameter value is used, normalized by the largest error of the possible values of one parameter and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst value for one parameter for each serie during phase $ 1 $ of the parameter search.\relax }}{83}{Each value in this table shows the average error when the corresponding parameter value is used, normalized by the largest error of the possible values of one parameter and finally subtracted by one. Therefore, each value shows a percentage of improvement with respect to the worst value for one parameter for each serie during phase $ 1 $ of the parameter search.\relax }{table.caption.81}{}}
\@writefile{lot}{\contentsline {table}{\numberline {B.4}{\ignorespaces The values of the parameters with the lowest average MAE on the validation set over three runs.\relax }}{83}{table.caption.82}\protected@file@percent }
\newlabel{tab:best_performing_para_phase1_model3}{{\M@TitleReference {B.4}{The values of the parameters with the lowest average MAE on the validation set over three runs.\relax }}{83}{The values of the parameters with the lowest average MAE on the validation set over three runs.\relax }{table.caption.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.4}{\ignorespaces Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM and \textit  {or}: best performing serie from phase one)\relax }}{84}{figure.caption.83}\protected@file@percent }
\newlabel{fig:sensitivity_model3}{{\M@TitleReference {B.4}{Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit  {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit  {r\_L}: regularization size of input weights of LSTM and \textit  {or}: best performing serie from phase one)\relax }}{84}{Results of the sensitivity analysis on the size of regulation parameter and the dropout rate with respect to the mean absolute error.(Legend: \textit {r\_r\_L}: regularization size of recurrent weight of LSTM, \textit {r\_L}: regularization size of input weights of LSTM and \textit {or}: best performing serie from phase one)\relax }{figure.caption.83}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.5}{\ignorespaces The evaluation of the error on the validation set in function of the learning rate size.\relax }}{85}{figure.caption.84}\protected@file@percent }
\newlabel{fig:learning_rate_model3}{{\M@TitleReference {B.5}{The evaluation of the error on the validation set in function of the learning rate size.\relax }}{85}{The evaluation of the error on the validation set in function of the learning rate size.\relax }{figure.caption.84}{}}
\@setckpt{app-B}{
\setcounter{page}{86}
\setcounter{equation}{0}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{@memmarkcntra}{0}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{2}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{maxsecnumdepth}{2}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{96}
\setcounter{lastsheet}{102}
\setcounter{lastpage}{92}
\setcounter{figure}{5}
\setcounter{lofdepth}{1}
\setcounter{table}{4}
\setcounter{lotdepth}{1}
\setcounter{Item}{2}
\setcounter{Hfootnote}{0}
\setcounter{memhycontfloat}{0}
\setcounter{Hpagenote}{0}
\setcounter{bookmark@seq@number}{31}
\setcounter{parentequation}{0}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{0}
\setcounter{algocfproc}{0}
\setcounter{algocf}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{3}
\setcounter{subtable}{0}
\setcounter{section@level}{2}
}
