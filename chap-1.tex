\chapter{Basic data analysis}
\label{cha:1}
In this chapter details of the dataset are introduced and a basic analysis is performed. This includes assessing missing data, seasonality,  influence of temperature and household data, comparing weekdays and weekends, applying an ARIMA model for forecasting.

% Look at the comments during the meeting 'writing the WTK thesis'.
% Make sure that write every part sufficient substantiated.

\section{Introduction to dataset}
% how the dataset is made up --> use the information of the competition.
The data that is used in this thesis is made available for the \href{https://ieee-dataport.org/competitions/ieee-cis-technical-challenge-energy-prediction-smart-meter-data}{IEEE-CIS technical challenge on energy prediction from smart data}. It consists out of data from smart meters about the 1/2 hour granulated electricity consumption of $3248$ households located in the United Kingdom in the year $2017$. Each smart meter collected thus a total of $17520$ measurements that are performed by the the leading international energy provider, E.ON UK plc. Not all the $3248$ smart meters consist of full data as can be seen in Figure \ref{fig:amountNaN} in appendix \ref{app:A}. It can be clearly seen that there are $12$ steps in the amount of missing values. This is because the available data ranges from one month (only December) to a full year of data. This acknowledges that customers may have joined at different times during the year. Additionally, missing values are introduced due to errors in sending/receiving from smart meters.\\
Next to the electricity consumption of the different households, also information is available about the average, minimum and maximum temperature of the day on the location of the smart meter. This data is available at a daily resolution. Also, through voluntary surveys, incomplete information is collected about $2143$ smart meters. This concerns e.g. dwelling type, number of occupants, number of bedrooms etc. Table \ref{tab:attributes} displays all the attributes in appendix \ref{app:A}.\\

%After substituting the missing values as discussed in \ref{s:missing_data}, all the available weeks are averaged out over all the $270$ smart meters that contain a full year of measurements. The result is given by Figure \ref{fig:averaged_week}. A difference that with belgian load profiles is the consumption peak after midnight. This is due to the higher use of the electric storage heaters used in the UK. These systems store electric energy when the electric tariff is low e.g. overnight and releases the heat when the tariff is high.
%

\section{Removing outliers}
% some meters don't have a lot of missing values, but have very untraditional output. Two cases are looked into
% 1. big deviation from the average meter. 
% 2. a full day of zeros is included. 
% 3. the moving average changes spectaculair --> fundamental change in the energy consumption
% witch is hard to forecast for. (not consistent with a normal consumption pattern)
% Weird meters that are identified: 2985, 2984,
% Normal meters: 2979, 2982

\section{Preprocessing}
\subsection{Missing data} \label{s:missing_data}
As discussed above the consumption dataset contains additionally to the missing months also missing data due to sending/receiving errors of the smart meters. When this happens the data of the whole day is lost. Three methods to impute the missing values are compared. 


\subsection{Normalization of the data}
% normalize as done in ppt
Normalization is necessary because while absolute consumption differs, relative patterns of human behaviour are more similar.\cite{Lago2020} The patterns in the human behaviour is what a forecasting model is trying to predict and normalization contributes by avoiding the disturbance of different magnitudes in which this human pattern may occur.  

\subsection{Removing of fundamental changes in the consumption load}  
This occurs for example when an extra person lives in the house or when systems are installed that use a lot of electricity.
This changes are identified by looking at the maximum difference of the minimum and maximum consumption for each individual meter.



%\cite{NarjesFallah2018}

\section{Basic analysis}
% Aggregation of the different signals is necessary in order to be able to make predictions.
Finally, the average is taken over all the remaining $211$ time-series to obtain a single signal. A single consumption time-serie is too much subdued to complex and personal decisions that can explain increases or decreases of the consumption. It is extremely hard to capture all theses effects in a single model. By aggregation of the individual time-series by taking the average, this noisy individual behaviour is mitigated. The aggregated signal is now modelled and the increase or decrease of the consumption can be explained by a small set of variables. The aggregated signal can be seen as a ``virtual'' distribution substation as discussed in \cite{Hoverstad2015}. Typical variables used in a forecasting model are: past electricity consumption loads, weather information, calendar information and error-correction terms \cite{loadforecastingmoor}.

\subsection{Seasonality}
% plot the moving average of the year. Clearly see the impact of the summer and winter.
% This is a trend that can be taken into account when predicting.

%To conclude, we find that all the forecasting algorithms
%considered in this paper produce more accurate forecasts
%when combined with a preprocessing stage that extracts the
%seasonality before forecasting, compared to applying the same
%algorithms directly on the raw data. \cite{Hoverstad2015}



\subsection{Influence of temperature}
% use the correlations. https://realpython.com/numpy-scipy-pandas-correlation-python/
% resample the consumption to daily and then apply some of the correlation techniques. 

\subsection{Influence of household data}
% try to investigate which attributes are the most important drivers of the consumptionload. Attributes with
% a low influence can be ignored.


\subsection{Comparing weekdays with weekends}

\subsection{Impact of holidays}
% important that look at holidays in the UK. In paper \cite{Hoverstad2015} all the holydays are subsitued by the same day the next week and the previous week. It is possible to look at all the holidays, normalize them concerning temperature and try to get a seasonality model. 



\section{ARIMA}
% Idea is to use the simple ARIMA model as a base line forecasting model. 
% see datacamp and youtube Lola
% ARIMA assumes stationary data.
Assumptions of ARIMA...

\textbf{Stationarity}\\
% https://machinelearningmastery.com/remove-trends-seasonality-difference-transform-python/
When data is modelled it is assumed that the statistics of the data are consistent or stationary. This means the mean and standard deviation is not changing in time. However, because time series are often subdued to a trend or seasonality this assumption of stationarity is violated. In order to model not stationary observations by a stationary model as ARIMA, trends and seasonal effects should be removed. A way to check the stationarity of your observations, the ``Dicky-Fuller test'' can be used.
A way to remove non-stationarity is by using ``Difference Transform''. Here the trend and seasonality is subtracted from the observations leaving behind a stationary dataset.


\section{Conclusion}
The final section of the chapter gives an overview of the important results
of this chapter. This implies that the introductory chapter and the
concluding chapter don't need a conclusion.




%Please don't abuse enumerations: short enumerations shouldn't use
%``\verb|itemize|'' or ``\texttt{enumerate}'' environments.
%So \emph{never write}: 
%\begin{quote}
%	The Eiffel tower has three floors:
%	\begin{itemize}
%		\item the first one;
%		\item the second one;
%		\item the third one.
%	\end{itemize}
%\end{quote}
%But write:
%\begin{quote}
%	The Eiffel tower has three floors: the first one, the second one, and the
%	third one.
%\end{quote}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
