Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Arenz2017,
abstract = {Inverse optimal control, also known as inverse reinforcement learning, is the problem of recovering an unknown reward function in a Markov decision process from expert demonstrations of the optimal policy. We introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality, and is suitable for large, continuous domains where even computing a full policy is impractical. By using a local approximation of the reward function, our method can also drop the assumption that the demonstrations are globally optimal, requiring only local optimality. This allows it to learn from examples that are unsuitable for prior methods.},
author = {Arenz, Oleg},
doi = {10.1007/978-1-4899-7687-1_100221},
journal = {Encyclopedia of Machine Learning and Data Mining},
pages = {678--678},
title = {{Inverse Optimal Control}},
year = {2017}
}
@article{Bae2019,
abstract = {The convergence of mechanical, electrical, and advanced ICT technologies, driven by artificial intelligence and 5G vehicle-to-everything (5G-V2X) connectivity, will help to develop high-performance autonomous driving vehicles and services that are usable and convenient for self-driving passengers. Despite widespread research on self-driving, user acceptance remains an essential part of successful market penetration; this forms the motivation behind studies on human factors associated with autonomous shuttle services. We address this by providing a comfortable driving experience while not compromising safety. We focus on the accelerations and jerks of vehicles to reduce the risk of motion sickness and to improve the driving experience for passengers. Furthermore, this study proposes a time-optimal velocity planning method for guaranteeing comfort criteria when an explicit reference path is given. The overall controller and planning method were verified using real-time, software-in-the-loop (SIL) environments for a real-time vehicle dynamics simulation; the performance was then compared with a typical planning approach. The proposed optimized planning shows a relatively better performance and enables a comfortable passenger experience in a self-driving shuttle bus according to the recommended criteria.},
author = {Bae, Il and Moon, Jaeyoung and Seo, Jeongseok},
doi = {10.3390/electronics8090943},
issn = {20799292},
journal = {Electronics (Switzerland)},
keywords = {Automated shuttle service,Autonomous vehicle,Driving comfort,Public transportation,Self-driving shuttle bus,Software-in-the-loop (SIL)},
number = {9},
pages = {1--13},
title = {{Toward a comfortable driving experience for a self-driving shuttle bus}},
volume = {8},
year = {2019}
}
@article{Ziebart2008,
abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Prob- lems. This approach reduces learning to the problem of re- covering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behav- ior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real- world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories. Introduction},
author = {{Brian D. Ziebart, Andrew Maas, J.Andrew Bagnell}, and Anind K. Dey},
doi = {10.1007/978-3-662-49390-8_64},
isbn = {9783662493892},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {6},
title = {{Maximum Entropy Inverse Reinforcement Learning Brian}},
year = {2008}
}
@phdthesis{Bellem,
author = {Bellem, Hanna},
pages = {115},
title = {{Comfort in Automated Driving : Analysis of Driving Style Preference in Automated Driving}},
year = {2018}
}
@article{Abbeel2004,
abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our. algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
author = {Abbeel, Pieter and Ng, Andrew Y.},
doi = {10.1145/1015330.1015430},
isbn = {1581138385},
journal = {Proceedings, Twenty-First International Conference on Machine Learning, ICML 2004},
pages = {1--8},
title = {{Apprenticeship learning via inverse reinforcement learning}},
year = {2004}
}
@phdthesis{Mercy2018,
author = {Mercy, Tim},
number = {September},
title = {{Spline-Based Motion Planning for Autonomous Mechatronic Systems}},
year = {2018}
}
@phdthesis{Yankov,
abstract = {Path planning and control of autonomous vehicles in dynamic environments can be a challenging problem, as a result of complex vehicle dynamics and constraints, as well as the existence of moving obstacles. Typical trajectory planners involve a high level decision making approach, using graph searches and a prior knowledge of the environment. Other approaches, attempt to use Potential Field (PF) functions as a basis for describing a goal within the environment, applied primarily to the field of robotics. This thesis aims to develop a method for path planning using online lane detection data from sensors. A Model Predictive Control (MPC) framework is proposed to fulfill this task. The path planner uses a simple dynamic model in combination with PF functions in order to create a feasible trajectory though the environment. To ensure safety around obstacles, vehicles and other actors are modeled as a potential field. A fast MPC controller is developed for path following. The two MPCs are then integrated in a hierarchical control structure. The method has been simulated in different scenarios and validated using a 15 degrees of freedom car model},
author = {Yankov, Kaloyan},
title = {{Potential Field Based Model Predictive Control for Autonomous Vehicle Motion Planning and Control}}
}
@phdthesis{Eindhoven2019,
address = {Eindhoven},
author = {{Yusof N.B.M}},
isbn = {9789038648088},
keywords = {technical university  eindhoven},
number = {2019},
title = {{Comfort in Autonomous Car : Mitigating Motion Sickness by Enhancing Situation Awareness through Haptic Displays Nidzamuddin Md . Yusof}},
year = {2019}
}
@article{Elbanhawi2015,
abstract = {The prospect of driverless cars wide-scale deployment is imminent owing to the advances in robotics, computational power, communications, and sensor technologies. This promises highway fatality reductions and improvements in traffic and fuel efficiency. Our understanding of the effects arising from commuting in autonomous cars is still limited. The novel concept of the loss of driver controllability is introduced here. It requires a reassessment of vehicle's comfort criteria. In this review paper, traditional comfort measures are examined and autonomous passenger awareness factors are proposed. We categorize path-planning methods in light of the offered factors. The objective of the review presented in this article is to highlight the gap in path planning from a passenger comfort perspective and propose some research solutions. It is expected that this investigation will generate more research interest and bring innovative solutions into this field.},
author = {Elbanhawi, Mohamed and Simic, Milan and Jazar, Reza},
doi = {10.1109/MITS.2015.2405571},
issn = {19391390},
journal = {IEEE Intelligent Transportation Systems Magazine},
number = {3},
pages = {4--17},
title = {{In the Passenger Seat: Investigating Ride Comfort Measures in Autonomous Cars}},
volume = {7},
year = {2015}
}
@article{Powers,
abstract = {Recently introduced RGB-D cameras are capable of providing high quality synchronized videos of both color and depth. With its advanced sensing capabilities, this technology represents an opportunity to dramatically increase the capabilities of object recognition. It also raises the problem of developing expressive features for the color and depth channels of these sensors. In this paper we introduce hierarchical matching pursuit (HMP) for RGB-D data. HMP uses sparse coding to learn hierarchical feature representations from raw RGB-D data in an unsupervised way. Extensive experiments on various datasets indicate that the features learned with our approach enable superior object recognition results using linear support vector machines.},
author = {{Powers, C., Mellinger, D., Kushleyev, A., Kothmann, B., Kumar}, V.},
doi = {10.1007/978-3-319-00065-7},
isbn = {978-3-319-00064-0},
issn = {21530858},
journal = {ISER, June},
number = {287513},
pages = {515--529},
title = {{Experimental Robotics}},
url = {http://link.springer.com/10.1007/978-3-319-00065-7},
volume = {88},
year = {2013}
}
@article{Turner1999,
abstract = {Relationships between vehicle motion and passenger sickness have been investigated in a survey of 3256 passengers travelling on 56 mainland UK bus or coach journeys. Vehicle motion was measured throughout all journeys, yielding over 110 h of six-axis coach motion data from five types of coach and 17 different drivers. Overall, 28.4{\%} of passengers reported feelings of illness, 12.8{\%} reported nausea and 1.7{\%} reported vomiting during coach travel. Passenger nausea and illness ratings increased with increased exposure to lateral coach motion at low frequencies ({\textless} 0.5 Hz). Motion in other axes correlated less well with sickness, although there were some intercorrelations between the motions in the different axes. Sickness levels among passengers were greater with drivers who drove to produce higher average magnitudes of fore-and-aft and lateral vehicle motion. Nausea occurrence was greater on routes classified as being predominantly cross-country where magnitudes of lateral vehicle motion were significantly higher. Lateral motion and motion sickness increased from the front to the rear of each vehicle. No significant differences in sickness were found between the five different vehicle types used in the study. The applicability of a motion sickness dose model to these data is discussed.},
author = {Turner, Mark and Griffin, Michael J.},
doi = {10.1080/001401399184730},
isbn = {0014013991},
issn = {00140139},
journal = {Ergonomics},
keywords = {Acceleration,Driver,Frequency,Motion sickness,Nausea,Vehicle},
number = {12},
pages = {1646--1664},
title = {{Motion sickness in public road transport: The effect of driver, route and vehicle}},
volume = {42},
year = {1999}
}
@article{TongDuySon2019,
abstract = {This paper presents a novel control framework to handle safety-critical control for non-affine nonlinear systems. The proposed control development is considered to deal l with safety-critical aspects in autonomous vehicle driving. The safety constraints are guaranteed using control barrier function (CBF), which implies forward-invariance of a safe set. In particular, we focus on CBF that enforces strict statedependent high relative degree constraints for general nonlinear vehicle models. Moreover, the CBF safety constraints are incorporated into a nonlinear model predictive control (NMPC) framework. The advantage is twofold. Firstly, both vehicle driving safety and comfort performance can be improved. Secondly, it helps to reduce computational burden in real time MPC implementation. The proposed algorithm is validated and compared with conventional NMPC in several safetycritical scenarios including sudden objects and road boundaries avoidance, showing improvements in both safety and smooth driving. The validation is conducted based on a co-simulation of two softwares, Siemens Simcenter Amesim and Prescan, which simulate high fidelity vehicle dynamics and traffic environment, respectively},
author = {{Tong Duy Son}, Quan Nguyen},
number = {August},
pages = {7},
title = {{Safety-Critical Control for Non-affine Nonlinear Systems with Application on Autonomous Vehicle}},
url = {file:///home/daniele/Downloads/ADAS{\_}SafetyCriticalControl.pdf},
year = {2019}
}
@article{Meyers,
author = {Meyers, Johan},
title = {{Lecture notes: Numerical Modelling in Mechanical Engineering}},
year = {2018}
}
@article{Gianna1996,
abstract = {To investigate the effect of velocity, acceleration, and gradient of acceleration on self-motion perception, thresholds for detection of direction of whole-body interaural acceleration were determined for various stimulus profiles. For acceleration steps, acceleration thresholds at 67{\%} correct detection of motion direction were similar for eight normals (mean 4.84 cm/s2 (range 2.9-6.3), peak gradient = 22 cm/s2) and five labyrinthine- defective subjects (mean 5.65 cm/s2 (4.85-6.6), peak gradient = 25 cm/s2). Velocity thresholds were 7.93 cm/s for a proportion of correct responses of 73{\%} for normals and 9.67 cm/s for 69{\%} of correct detection for avestibular subjects. For linear and parabolic accelerations, high intersubject variability was observed both among nine normals and three labyrinthine- defective subjects. Mean normal and avestibular subjects' acceleration thresholds for 74{\%} of correct responses were respectively 12.1 cm/s2 (7.3- 20.4) and 16.4 cm/s2 (13.2-20) for a ramp with gradient of acceleration = 2.8 cm/s2, 19.2 cm/s2 (10.4-35.3) and 28.2 cm/s2 (21.4-32.8) for a ramp with gradient = 7.9 cm/s3 and 16.7 cm/s2 (10.5-25) and 20.6 cm/s2 (18.424.2) for a parabola with second derivative = 1.52 cm/s4. The corresponding velocity thresholds for normals were 21.2 cm/s (5.2-50.3), 22.0 cm/s (7-56.6), and 22.2 cm/s (9.5-43.7). The lowest thresholds were obtained for acceleration steps indicating that a high acceleration gradient facilitates motion perception. For linear and parabolic accelerations, motion perception seemed to follow an integration of acceleration, but a high intersubject variability was observed. For all stimuli, the range of thresholds for normals and avestibular subjects overlapped showing that detection of motion was not a sole prerogative of the otoliths but could also be performed using somatosensory cues.},
author = {Gianna, Claire and Heimbrand, S. and Gresty, M.},
doi = {10.1016/0361-9230(96)00140-2},
issn = {03619230},
journal = {Brain Research Bulletin},
keywords = {Linear,Otolith,Perception,Somatosensory,Vestibular},
number = {5-6},
pages = {443--447},
title = {{Thresholds for detection of motion direction during passive lateral whole-body acceleration in normal subjects and patients with bilateral loss of labyrinthine function}},
volume = {40},
year = {1996}
}
@misc{Panos_opti,
author = {Patrinos, Panos},
title = {{Optimization - Lecture notes}},
year = {2019}
}
@article{Palan2019,
abstract = {Our goal is to accurately and efficiently learn reward functions for autonomous robots. Current approaches to this problem include inverse reinforcement learning (IRL), which uses expert demonstrations, and preference-based learning, which iteratively queries the user for her preferences between trajectories. In robotics however, IRL often struggles because it is difficult to get high-quality demonstrations; conversely, preference-based learning is very inefficient since it attempts to learn a continuous, high-dimensional function from binary feedback. We propose a new framework for reward learning, DemPref, that uses both demonstrations and preference queries to learn a reward function. Specifically, we (1) use the demonstrations to learn a coarse prior over the space of reward functions, to reduce the effective size of the space from which queries are generated; and (2) use the demonstrations to ground the (active) query generation process, to improve the quality of the generated queries. Our method alleviates the efficiency issues faced by standard preference-based learning methods and does not exclusively depend on (possibly low-quality) demonstrations. In numerical experiments, we find that DemPref is significantly more efficient than a standard active preference-based learning method. In a user study, we compare our method to a standard IRL method; we find that users rated the robot trained with DemPref as being more successful at learning their desired behavior, and preferred to use the DemPref system (over IRL) to train the robot.},
archivePrefix = {arXiv},
arxivId = {1906.08928},
author = {Palan, Malayandi and Landolfi, Nicholas C. and Shevchuk, Gleb and Sadigh, Dorsa},
eprint = {1906.08928},
title = {{Learning Reward Functions by Integrating Human Demonstrations and Preferences}},
url = {http://arxiv.org/abs/1906.08928},
year = {2019}
}
@article{Kuderer2015a,
abstract = {It is expected that autonomous vehicles capable of driving without human supervision will be released to market within the next decade. For user acceptance, such vehicles should not only be safe and reliable, they should also provide a comfortable user experience. However, individual perception of comfort may vary considerably among users. Whereas some users might prefer sporty driving with high accelerations, others might prefer a more relaxed style. Typically, a large number of parameters such as acceleration profiles, distances to other cars, speed during lane changes, etc., characterize a human driver's style. Manual tuning of these parameters may be a tedious and error-prone task. Therefore, we propose a learning from demonstration approach that allows the user to simply demonstrate the desired style by driving the car manually. We model the individual style in terms of a cost function and use feature-based inverse reinforcement learning to find the model parameters that fit the observed style best. Once the model has been learned, it can be used to efficiently compute trajectories for the vehicle in autonomous mode. We show that our approach is capable of learning cost functions and reproducing different driving styles using data from real drivers.},
author = {Kuderer, Markus and Gulati, Shilpa and Burgard, Wolfram},
doi = {10.1109/ICRA.2015.7139555},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Autonomous Navigation,Learning and Adaptive Systems},
number = {June},
pages = {2641--2646},
title = {{Learning driving styles for autonomous vehicles from demonstration}},
volume = {2015-June},
year = {2015}
}
@misc{Daniel2018,
abstract = {The disclosure provides for a method for determining a route for passenger comfort and operating a vehicle according to the determined route . To start , a set of routes from a start location to an end location may be determined . Each route includes one or more portions . For each route of the set of routes , a total motion sickness value is determined based on a sway motion sickness value , a surge motion sickness value , and a heave motion sickness value for each of the given portions . The total motion sickness value for a route reflects a likelihood that a user will experience motion sickness while in a vehicle along the route . A route may then be selected from the set of routes based on the total motion sickness value of each route of the set of routes , and the vehicle may be maneuvered according to the selected route . 20},
author = {Daniel, Larner},
title = {{Method and system for determining and dynamically updating a route and driving style for passenger comfort - US Patent}},
volume = {2},
year = {2018}
}
@article{Levine2012,
abstract = {Inverse optimal control, also known as inverse reinforcement learning, is the problem of recovering an unknown reward function in a Markov decision process from expert demonstrations of the optimal policy. We introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality, and is suitable for large, continuous domains where even computing a full policy is impractical. By using a local approximation of the reward function, our method can also drop the assumption that the demonstrations are globally optimal, requiring only local optimality. This allows it to learn from examples that are unsuitable for prior methods. Copyright 2012 by the author(s)/owner(s).},
archivePrefix = {arXiv},
arxivId = {1206.4617},
author = {Levine, Sergey and Koltun, Vladlen},
eprint = {1206.4617},
isbn = {9781450312851},
journal = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
pages = {41--48},
title = {{Continuous inverse optimal control with locally optimal examples}},
volume = {1},
year = {2012}
}
@misc{Gillis2019,
author = {Gillis, Joris},
number = {November},
pages = {1--42},
title = {{Ya Coda course presentation}},
year = {2019}
}
@article{RPROP,
abstract = {A new learning algorithm for multilayer feedforward networks, RPROP, is proposed. To overcome the inherent disadvantages of pure gradient-descent, RPROP performs a local adaptation of the weight-updates according to the behavior of the error function. In substantial difference to other adaptive techniques, the effect of the RPROP adaptation process is not blurred by the unforeseeable influence of the size of the derivative but only dependent on the temporal behavior of its sign. This leads to an efficient and transparent adaptation process. The promising capabilities of RPROP are shown in comparison to other well-known adaptive techniques.},
author = {Riedmiller, Martin and Braun, Heinrich},
doi = {10.1109/icnn.1993.298623},
isbn = {0780312007},
journal = {1993 IEEE International Conference on Neural Networks},
pages = {586--591},
title = {{Direct adaptive method for faster backpropagation learning: The RPROP algorithm}},
year = {1993}
}
@misc{Prof.Amnon,
author = {{Prof. Amnon Shashua}},
keywords = {Autonomous Driving,Mobileye},
month = {mar},
title = {{Experience Counts, Particularly in Safety-Critical Areas | Intel Newsroom}},
url = {https://newsroom.intel.com/editorials/experience-counts-particularly-safety-critical-areas/{\#}gs.1wzemi},
urldate = {2020-04-01},
year = {2018}
}
@article{Kretzschmar2014,
abstract = {The problem of modeling the navigation behavior of multiple interacting agents arises in different areas including robotics, computer graphics, and behavioral science. In this paper, we present an approach to learn the composite navigation behavior of interacting agents from demonstrations. The decision process that ultimately leads to the observed continuous trajectories of the agents often also comprises discrete decisions, which partition the space of composite trajectories into homotopy classes. Therefore, our method uses a mixture probability distribution that consists of a discrete distribution over the homotopy classes and continuous distributions over the composite trajectories for each homotopy class. Our approach learns the model parameters of this distribution that match, in expectation, the observed behavior in terms of user-defined features. To compute the feature expectations over the high-dimensional continuous distributions, we use Hamiltonian Markov chain Monte Carlo sampling. We exploit that the distributions are highly structured due to physical constraints and guide the sampling process to regions of high probability. We apply our approach to learning the behavior of pedestrians and demonstrate that it outperforms state-of-the-art methods.},
author = {Kretzschmar, Henrik and Kuderer, Markus and Burgard, Wolfram},
doi = {10.1109/ICRA.2014.6907442},
isbn = {9781479936854},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {4015--4020},
publisher = {IEEE},
title = {{Learning to predict trajectories of cooperatively navigating agents}},
year = {2014}
}
@misc{Patrinos2019,
author = {Patrinos, Panos},
title = {{Model Predictive Control - Lecture Notes}},
year = {2019}
}
@misc{UniversityofWarwick2019,
author = {{University of Warwick}},
booktitle = {Science Daily},
title = {{Do passengers prefer autonomous vehicles driven like machines or like humans?}},
url = {https://www.sciencedaily.com/releases/2019/07/190708092850.htm},
urldate = {2020-04-03},
year = {2019}
}
@misc{AV,
author = {David, Emm},
keywords = {AUTONOMOUS VEHICLES,INTERNET OF THINGS},
title = {{Will autonomous vehicles be safe to use? | Cybersecurity {\&} Technology News | Secure Futures | Kaspersky}},
url = {https://www.kaspersky.com/blog/secure-futures-magazine/cybersecurity-autonomous-vehicles/28291/},
urldate = {2020-04-01}
}
